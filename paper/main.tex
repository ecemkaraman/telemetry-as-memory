\documentclass[10pt,conference]{article}

% ---------- Packages ----------
\usepackage[margin=1in]{geometry}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{float}
\usepackage{placeins}
\usepackage{subcaption} 
\usepackage[font=small,labelfont=bf]{caption} % bold "Figure X" + smaller caption font
\usepackage{capt-of}                           % enables \captionof{figure}{...} in minipage
\usepackage{cite} 
\setlist{nosep}

% Bibliography
\bibliographystyle{IEEEtran}

% Helpers
\newcommand{\tam}{\textsc{TAM}} % Telemetry-as-Memory

\title{Telemetry-as-Memory (TAM): Using Observability Pipelines to Train Adaptive AI Systems}
\author{Ecem Karaman}
\date{August 2025}

\begin{document}
\maketitle

\begin{abstract}
Observability pipelines such as \textbf{OpenTelemetry} and \textbf{Prometheus} collect logs, metrics, and traces at scale, but remain largely \textbf{passive}: they power dashboards and alerts rather than training adaptive systems \cite{opentelemetry2023spec,google2016sre}. As a result, operational AI models are retrained offline on stale data, leaving them brittle to drift \cite{gama2014conceptdrift} and vulnerable to poisoned telemetry \cite{carlini2017poisoning,barreno2006secureml,biggio2012poisoning}.

This work proposes \emph{Telemetry-as-Memory (TAM)}, a closed-loop framework that treats telemetry as an \emph{adaptive memory substrate}. TAM continuously ingests telemetry, assigns trust scores, and applies \textbf{online learning} \cite{montiel2020river,bifet2007adwin}, enabling secure, low-latency adaptation under non-stationarity. The design integrates drift detection, trust-weighted updates, and lightweight decision policies.

We evaluate TAM under three stressors: (1) \textbf{Concept Drift} --- recovery within $\sim$27 ticks vs.\ 300 for baselines ($\sim$10$\times$ faster) with $\sim$4\% false positives; (2) \textbf{Poisoned Logs} --- trust scoring blocks adversarial injections, preserving accuracy; and (3) \textbf{Novel Incidents} --- unseen ``disk full'' patterns adapted within tens of ticks, avoiding blind spots. Results show that treating observability as adaptive memory enables \textbf{secure, self-healing AIOps pipelines} resilient to drift, poisoning, and novel incidents.
\end{abstract}

\section{Introduction}
Cloud-native systems emit vast telemetry streams, yet observability pipelines still treat this data primarily as diagnostic output for dashboards and alerts \cite{opentelemetry2023spec,grafana2023adaptive}. While frameworks (e.g., OpenTelemetry, Prometheus) standardize collection and monitoring, they do not leverage telemetry as direct input for adaptive learning. At the same time, operational ML remains largely offline, with models retrained only on historical data—leaving systems brittle under drift \cite{gama2014conceptdrift}, slow to detect novel incidents, and vulnerable to telemetry poisoning \cite{carlini2017poisoning,barreno2006secureml,biggio2012poisoning}.
\emph{Telemetry-as-Memory (TAM)} closes this gap by reframing observability data as adaptive memory. Instead of powering only visualizations, TAM enables models to update incrementally and adapt continuously.

\paragraph*{Contributions}
\begin{enumerate}
    \item \textbf{Framework}: A closed-loop design where telemetry streams directly drive online learning.  
    \item \textbf{Security}: Trust-weighted updates mitigate poisoning and drift exploitation in adaptive feedback loops.  
    \item \textbf{Evidence}: Synthetic Kubernetes experiments validate TAM across three stressors—$\sim$10$\times$ faster recovery under drift, bounded false positives under poisoning, and rapid adaptation to novel incidents.  
\end{enumerate}

This shifts observability from ``monitor and alert'' toward \textbf{observe, learn, and act}.

\section{Background and Related Work}

\subsection{Observability Pipelines}
Modern observability frameworks (e.g., OpenTelemetry, Prometheus) standardize collection of logs, metrics, and traces at scale \cite{opentelemetry2023spec}. They excel at monitoring and visualization but keep telemetry largely \emph{passive}. Extensions such as \textbf{Grafana Adaptive Metrics} reduce storage and cost overheads \cite{grafana2023adaptive}, yet none treat telemetry as a substrate for adaptive learning. This reflects long-standing critiques in reliability engineering, where observability has been viewed as operator-facing signals rather than inputs to self-adaptive systems \cite{google2016sre,bodik2010fingerprinting}.
\subsection{Online and Meta-Learning}
\textbf{Online learning} updates incrementally with each new data point, supporting non-stationary environments \cite{domingos2000vfdt,bifet2007adwin,montiel2020river,gama2014conceptdrift}. \textbf{Meta-learning} extends this by optimizing for fast adaptation itself \cite{finn2019online}. Operational explorations include \textbf{MAML-KAD} for anomaly detection \cite{duan2024mamlkad}, \textbf{SAM} for anomaly transfer across clouds \cite{jha2024sam}, and \textbf{OMLog} for log streams \cite{tian2024omlog}. Despite promise, integration into production-grade observability pipelines remains rare.

\subsection{Self-Healing and AIOps Systems}
AIOps spans log anomaly detection (DeepLog, LogAnomaly) to RL-based remediation. Recent work combines \textbf{LLMs and RL} for fault remediation \cite{yang2025llmrl}, but most systems rely on static anomaly detectors or rules, limiting adaptivity under drift. The long-standing autonomic computing vision \cite{kephart2003autonomic} calls for closed-loop, self-managing systems, but practical adoption remains fragmented.

\subsection{Gap Analysis}
Prior work solves isolated parts:
\begin{itemize}[leftmargin=*]
  \item Observability pipelines \emph{collect and visualize} but stop short of learning.  
  \item Online/meta-learning enables rapid adaptation but is not embedded in operational telemetry.  
  \item AIOps prototypes automate response but lack trust, explainability, and adversarial robustness.  \\
\end{itemize}

\noindent This work bridges the gap with \emph{Telemetry-as-Memory (TAM)}, unifying:
\begin{itemize}[leftmargin=*]
  \item \textbf{Real-time online learning},  
  \item \textbf{Trust-scored, security-gated updates},  
  \item \textbf{Optional long-term semantic recall (future extension)}, and  
  \item \textbf{Governance and explainability layers}.  
\end{itemize}

\section{Methodology and System Design}
The \emph{Telemetry-as-Memory (TAM)} framework is a modular, closed-loop pipeline that converts raw observability streams into adaptive signals for online learning and secure decision-making. Figure~\ref{fig:tam-arch} illustrates its six layers.

\textbf{Ingestion.} Logs capture system events; metrics record CPU/latency; traces encode request paths. Collection uses OpenTelemetry or cloud-native collectors, streamed via Kafka/EventHub/Kinesis for scalability and buffering.

\textbf{Preprocessing and Trust Scoring.} Raw telemetry is deduplicated, schema-validated, and PII-masked. Each event receives a \emph{trust score} based on source validity, schema compliance, and anomaly likelihood—reliable signals weigh strongly, while low-trust events are downweighted or dropped.  

\textbf{Featurization and Memory.} Telemetry is encoded as features: rolling aggregates/deltas for metrics; token flags, hashing vectorizers, or semantic embeddings (e.g., Sentence-BERT) for logs; and span counts or service-graph encodings for traces. A hierarchical memory balances short-term sliding windows with long-term vector stores (e.g., FAISS, Pinecone) for retrieval-augmented learning.  

\textbf{Adaptive Learning.} Models update incrementally using streaming algorithms (e.g., logistic regression, Hoeffding trees in River). Drift detectors (e.g., ADWIN) flag distribution shifts and trigger reweighting. Trust-weighted updates limit poisoned inputs, while security gates block unsafe updates—ensuring adaptivity without instability.  

\textbf{Inference and Action.} The learner combines current features with recalled incidents to predict outcomes and trigger actions (e.g., restart service, scale deployment, block IP). High-impact actions pass through gates or human review before orchestration (e.g., Kubernetes runbooks). Each action is logged with its trust scores and context for analysis.  

\textbf{Governance and Explainability.} TAM supports approval gates, audit trails, and optional interpretability layers (e.g., SHAP, LIME) to provide rationales for updates and enable rollback of unreliable changes, balancing auditability with throughput.  

Together, these layers transform observability from a passive diagnostic tool into an \emph{adaptive memory substrate}, continuously closing the loop between observation, learning, and action.
\vspace{-0.5em}
\begin{figure*}[b]
  \centering
  \includegraphics[height=0.98\textheight]{figures/tam.png}
    \caption{\textbf{System Design Architecture.} Telemetry flows through six layers—ingestion, trust scoring, featurization/memory, adaptive learning, inference/action, and governance/explainability—forming a closed adaptive loop.}  \label{fig:tam-arch}
\end{figure*}
\FloatBarrier

\section{Threat Model}
We assume adversaries can inject or manipulate telemetry but lack full system control. 
Key risks include:

\begin{itemize}[leftmargin=*]
  \item \textbf{Poisoning:} fake logs/metrics corrupt model memory or bias updates.
  \item \textbf{Drift Exploitation:} gradual distribution shifts normalize malicious activity.
  \item \textbf{Adversarial Inputs:} crafted logs/traces mislead anomaly detectors.
  \item \textbf{Feedback Abuse:} induced failures cause harmful adaptive loops (``drift spirals'').
  \item \textbf{Privilege Escalation:} exploiting automated actions (e.g., scaling, firewall rules) for leverage.
\end{itemize}

\noindent \textbf{Mitigations:} trust scoring, drift detection, security gates, and audit trails.
\section{Evaluation}

\subsection{Goals and Hypotheses}
\begin{itemize}[leftmargin=*]
  \item \textbf{H1 (Adaptation speed).} Closed-loop TAM recovers from concept drift within an order of magnitude fewer ticks than offline retraining.
  \item \textbf{H2 (Safety under attack).} Trust scoring reduces the influence of poisoned telemetry, keeping false positives $<5\%$.
  \item \textbf{H3 (Novel incident handling).} Memory-augmented learning adapts to unseen patterns (e.g., ``disk full'') without retraining.
\end{itemize}

\subsection{Experimental Setup}
\begin{itemize}[leftmargin=*]
  \item \textbf{Environment.} 
    \begin{itemize}
      \item CPU\%: sinusoidal baseline with injected spikes.  
      \item Error rate: step increase at $t \approx 300$ to induce drift.  
      \item Logs: INFO/WARN/ERROR aligned with error conditions.  
    \end{itemize}
  \item \textbf{Models.} 
    \begin{itemize}
      \item \emph{Baseline:} Offline logistic regression retrained every $N$ ticks (common in AIOps).  
      \item \emph{Closed-loop \tam:} River-based online logistic regression with trust-weighted updates.  
    \end{itemize}
  \item \textbf{Detection.} ADWIN for drift detection; trust scores to downweight unverified telemetry.  
  \item \textbf{Policy.} Static thresholds; adaptive bandit-style left for future work.  
  \item \textbf{Reproducibility.} Experiments deterministic (fixed seeds); variance across runs $\sim$$<5\%$ latency, $<2\%$ FP.  
\end{itemize}

\subsection{Scenarios}
\begin{itemize}[leftmargin=*]
  \item \textbf{Scenario 1 (H1 – Concept Drift).} Error rate increases at $t\!\approx\!300$, making the baseline decision boundary stale.  
  \item \textbf{Scenario 2 (H2 – Poisoned Logs).} Inject synthetic \texttt{ERROR} messages from untrusted sources to test trust scoring.  
  \item \textbf{Scenario 3 (H3 – Novel Incident).} Introduce unseen ``disk full'' logs to test retrieval-augmented adaptation.  
\end{itemize}

\subsection{Metrics}
\begin{itemize}[leftmargin=*]
  \item \textbf{Primary.} Adaptation latency (ticks to recover) and false positive rate (spurious actions).  
  \item \textbf{Secondary.} False negatives, drift detection recall, action ROI, recovery iterations.  
  \item \textbf{Emphasis.} Primary metrics prioritized in analysis; secondary metrics defined but not collected in this prototype.  
\end{itemize}
\subsection{Results}
For each scenario, we show: (i) accuracy recovery and (ii) FP trade-offs.  

\emph{All results are averaged over 5 seeds. Variance across runs was modest ($<5\%$ latency, $<2\%$ FP).}


\subsubsection*{Scenario 1: Concept Drift (H1)}
\vspace{-2.5em}
\begin{figure*}[!htb]
  \centering
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/drift_acc.png}
    \captionof{figure}{\textbf{H1-Accuracy under drift.} Baseline collapses after $t\!\approx\!300$ and recovers only at retrain ($t\!\approx\!600$). Closed-loop adapts within $\sim$27 ticks, validating H1.}
  \end{minipage}\hfill
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/drift_fp.png}
    \captionof{figure}{\textbf{H1-FP rate.} Baseline $\approx0\%$ (inactive). Closed-loop $\sim$4\% FP, an acceptable safety–adaptivity trade-off.}
  \end{minipage}
\end{figure*}
\FloatBarrier

\subsubsection*{Scenario 2: Poisoned Logs (H2)}
\vspace{-2.5em}
\begin{figure*}[!htb]
  \centering
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/poison_acc.png}
    \captionof{figure}{\textbf{H2 – Accuracy under poisoning.} Baseline fails on fake \texttt{ERROR}s, while Closed-loop trust scoring downweights untrusted sources and preserves accuracy (supports H2).}
  \end{minipage}\hfill
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/poison_fp.png}
    \captionof{figure}{\textbf{H2–FP rate.} Baseline FP spikes under poisoning. Closed-loop keeps FP $<5\%$ via trust-scored filtering.}
  \end{minipage}
\end{figure*}
\FloatBarrier

\subsubsection*{Scenario 3: Novel Incident (H3)}
\vspace{-2.5em}
\begin{figure*}[!htb]
  \centering
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/novel_acc.png}
    \captionof{figure}{\textbf{H3–Accuracy for unseen ``disk full''.} Baseline fails until retrain. Closed-loop adapts within tens of ticks via online updates, confirming H3.}
  \end{minipage}\hfill
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/novel_fp.png}
    \captionof{figure}{\textbf{H3–FP rate.} Closed-loop incurs moderate FP while adapting, but recovers within <40 ticks despite moderate FP. Baseline inert until retrain.}
  \end{minipage}
\end{figure*}
\FloatBarrier

\section{Discussion}

\subsection{Engineering Challenges}
\begin{itemize}[leftmargin=*]
  \item \textbf{Drift vs.\ Noise.} Differentiating persistent drift from transient spikes is nontrivial; adaptive learners risk overreacting.  
  \item \textbf{Telemetry Bloat.} High-cardinality metrics/logs demand efficient sampling and sketching to avoid $O(n)$ blowup in memory and compute.  
  \item \textbf{Security vs.\ Adaptivity.} Online learning favors fast updates, but security requires stability and reproducibility. \tam mitigates this via trust scoring and drift detection. This tension maps directly to our threat model.  
\end{itemize}

\subsection{Trade-offs}
\begin{itemize}[leftmargin=*]
  \item \textbf{Adaptivity vs.\ Stability.} Faster recovery risks oscillation; mitigated via weighted updates and drift detection.  
  \item \textbf{Responsiveness vs.\ Reliability.} Real-time actioning yields $\sim$4\% FP in drift/novel scenarios but avoids 300+ ticks of downtime.  
  \item \textbf{Transparency vs.\ Overhead.} Explainability gates (LIME, SHAP) improve auditability but add $\sim$15–20\% runtime overhead \cite{lundberg2017shap,ribeiro2016lime}.  
\end{itemize}

\subsection{Broader Implications}
\begin{itemize}[leftmargin=*]
  \item \textbf{From Alerts to Agents.} \tam reframes observability as \emph{adaptive memory}, moving pipelines toward self-learning agents.  
  \item \textbf{Resilient Infrastructure.} Results show resilience: recovery within $\sim$27 ticks post-drift, FP $<5\%$ under poisoning, and rapid adaptation to unseen incidents.  
  \item \textbf{Security Observability.} Trust-scored telemetry demonstrates feasibility of adaptive, adversary-resistant pipelines.  
  \item \textbf{Regulatory.} Adaptive AI raises governance demands: rollback, explainability, and human oversight.  
\end{itemize}

\subsection{Limitations}

\begin{itemize}[leftmargin=*]
  \item \textbf{Synthetic workloads.} Evaluation used simulated Kubernetes telemetry only; real production workloads may introduce noise and variability.
  \item \textbf{Model scope.} Results are limited to logistic regression; sequence or embedding-based models (e.g., RNNs, Transformers) remain future work.
  \item \textbf{Governance.} Features such as human-in-the-loop approval and rollback were defined conceptually but not implemented in the current prototype.
  \item \textbf{Metric scope.} Secondary metrics (e.g., drift recall, action ROI) were defined but not collected; future work should report them.
\end{itemize}

\section{Conclusion and Future Work}
This work introduced \textbf{Telemetry-as-Memory (TAM)}, reframing observability pipelines as 
\emph{adaptive memory systems}. By continuously ingesting telemetry, trust-weighting inputs, 
and applying online learning, TAM closes the loop between \emph{observe, learn, and act}.

\subsection*{Summary of Findings}
Across three scenarios, TAM demonstrated:
\begin{itemize}[leftmargin=*]
  \item \textbf{Concept Drift:} Recovery within $\sim$27 ticks vs.\ 300 for baselines 
        ($\sim$10$\times$ faster), with false positives bounded to $\sim$4\%.
  \item \textbf{Poisoned Logs:} Trust scoring blocked adversarial \texttt{ERROR} injections, 
        maintaining stable accuracy where baselines degraded.
  \item \textbf{Novel Incidents:} Unseen ``disk full'' patterns were adapted within tens of ticks, 
        validating faster response to new issues.
\end{itemize}

Together, these results validate secure, trust-weighted online learning in AIOps. 
Remaining risks include poisoning, drift manipulation, and unsafe automation; 
trust scoring and drift detection mitigate these, while governance and explainability remain future extensions.

\subsection*{Future Work}
Several directions remain:
\begin{itemize}[leftmargin=*]
  \item \textbf{Deployment:} Validate scalability by integrating TAM into production observability stacks 
        (Kubernetes + OpenTelemetry).
  \item \textbf{Explainability \& Governance:} Add SHAP/LIME gates, human-in-the-loop approval, 
        and rollback strategies for high-risk actions.
  \item \textbf{Richer Models:} Explore sequence-based/embedding models (RNNs, Transformers) 
        and vector databases for semantic recall.
  \item \textbf{Multi-Agent Systems:} Study federated or swarm-style agents that share telemetry 
        as distributed memory.
\end{itemize}

By treating observability as adaptive memory, TAM lays groundwork for secure, explainable, 
and self-healing AIOps systems capable of keeping pace with dynamic cloud-native environments.
\section*{Appendix A: Implementation Details}
The \emph{TAM} prototype is implemented as a modular Python package, available at \href{https://github.com/ecemkaraman/telemetry-as-memory}{\textbf{GitHub Repository}}. Whereas Section~3 described the pipeline conceptually, this appendix documents the code-level modules.

\subsubsection*{Core Modules}
\begin{itemize}[leftmargin=*]
  \item \texttt{src/tam/telemetry.py} — Implements a synthetic Kubernetes-like telemetry generator with support for 
        concept drift, poisoned logs, and novel incident injection.
  \item \texttt{src/tam/baseline.py} — Implements the offline retraining baseline (logistic regression retrained every $N$ ticks).
  \item \texttt{src/tam/online.py} — Implements the closed-loop learner with trust-weighted online updates and drift detection (ADWIN).
  \item \texttt{src/tam/trust.py} — Provides trust scoring functions based on source validity, schema compliance, and anomaly likelihood.
  \item \texttt{src/tam/policy.py} — Encodes threshold-based and bandit-style decision policies that map predictions to automated actions.
  \item \texttt{src/tam/metrics.py} — Defines evaluation metrics: rolling accuracy, adaptation latency, false-positive rate, recovery iterations.
\end{itemize}

\subsubsection*{Experiment Utilities}
\begin{itemize}[leftmargin=*]
  \item \texttt{src/cli/run\_eval.py} — Experiment runner for all scenarios (concept drift, poisoned logs, novel incident).
  \item \texttt{scripts/plot\_results.py} — Post-processing utilities for aggregating results and generating evaluation figures.
\end{itemize}

\subsubsection*{Execution Flow}
The overall flow is as follows:
\begin{enumerate}[leftmargin=*]
  \item \textbf{Telemetry Generation:} synthetic logs, metrics, and traces (\texttt{telemetry.py}).
  \item \textbf{Preprocessing and Trust Scoring:} schema enforcement and anomaly weighting (\texttt{trust.py}).
  \item \textbf{Featurization and Memory:} rule-based tokens, hashing vectorizers, embeddings (\texttt{features.py}, \texttt{telemetry.py}).
  \item \textbf{Adaptive Learning:} online/meta-learning models with drift detection (\texttt{online.py}, \texttt{drift.py}).
  \item \textbf{Inference and Action:} predictions mapped to orchestration calls (\texttt{policy.py}).
  \item \textbf{Governance and Explainability:} approval gates, audit logging, and explainability checks (\texttt{online.py}, \texttt{metrics.py}).
  \item \textbf{Experiment Execution:} scenarios run via \texttt{run\_eval.py}, results stored as CSV and JSON.
  \item \textbf{Visualization:} evaluation figures produced by \texttt{plot\_results.py}.
\end{enumerate}

\noindent Figure~\ref{fig:code_flow} illustrates the code-level flow and module interactions.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/code_flow.png}
  \caption{\textbf{Implementation Flow (Code-Level).} Mapping of the conceptual pipeline (Figure~\ref{fig:tam-arch}) to concrete modules in the \tam prototype. Blue = online learner, Gray = offline baseline, Green = policy, Purple = metrics/IO, Dashed = optional or internal. Entry points orchestrate experiments (\texttt{run\_eval.py}) and visualization (\texttt{plot\_results.py}).}
    \label{fig:code_flow}
\end{figure}

\bibliography{references}
\end{document}
